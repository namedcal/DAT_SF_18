{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence and Word Segmentation\n",
    "\n",
    "The first step in NLP is cutting text into its constituents. Namely, sentences and words. Let's see how well we can perform this task in base python.\n",
    "\n",
    "**DO NOT worry about writing efficient code.** We're just practicing NLP principles.\n",
    "\n",
    "It will useful to know the String methods! These are one of the most useful features of Python for text processing!\n",
    "\n",
    "https://docs.python.org/2/library/stdtypes.html#string-formatting-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Let's start with sentence segmentation. English typically end with a period, exclamation, or question mark. Let's start easy."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJy\\nZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You do: \n",
    "- define a function to split a string into sentences.\n",
    "- If you're familiar with regexes, feel free to use the re module"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentencer(text):\n",
<<<<<<< HEAD
    "   '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "   \n",
    "   sentences = []\n",
    "   substring = ''\n",
    "   for c in text:\n",
    "       if c in ('.', '!', '?'):\n",
    "           sentences.append(substring + c)\n",
    "           substring = ''\n",
    "       else:\n",
    "           substring += c\n",
    "\n",
    "   if substring:\n",
    "       sentences.append(substring)\n",
    "\n",
    "   return sentences"
=======
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentences"
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer(easy_text)) == easy_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "What about cases where periods denote abbreviations? This time, try to do the same splits, but accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "med_text = \"My name is Dr. Lee. There is also a Mrs. Lee. Actually, there are tons! They're other people's wives.\"\n",
    "med_split_text = [\"My name is Dr. Lee.\",\n",
    "                  \"There is also a Mrs. Lee.\",\n",
    "                  \"Actually, there are tons!\",\n",
    "                  \"They're other people's wives.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "\n",
    "def sentencer2(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
<<<<<<< HEAD
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        elif c == '.':\n",
    "            if substring[-3:].lower() == 'mrs' or substring [-2:].lower() in ['dr','mr','ms']:\n",
    "                substring += c\n",
    "                \n",
    "        else:\n",
    "            substring += c\n",
    "\n",
    "    if substring:\n",
    "        sentences.append(substring)\n",
    "\n",
    "    if \n",
    "\n",
    "   return sentences\n",
=======
    "    # FILL IN CODE\n",
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentencer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b7932a1b2a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test your function by running this cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentencer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmed_split_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'Congratulations!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentencer2' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer2(med_text)) == med_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
<<<<<<< HEAD
    "    print 'Sorry, try ahardgain!'\n",
=======
    "    print 'Sorry, try again!'\n",
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer2(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Exercise: sentence segmentation continued\n",
    "\n",
    "Abbreviations like 'a.k.a.' are harder to accommodate. This one is quite challenging, so you can skip it if you want to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('VHJ5IGFsbG93aW5nIHRoZSBzcGxpdHMgb24gdGhlIHBlcmlvZHMsIGJ1dCB0aGVuIHJlYXR0YWNo\\naW5nIGlmIHRoZSBuZXh0IHNlbnRlbmNlIGlzIG9ubHkgb25lIGNoYXJhY3RlciBsb25n\\n')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "hard_text = \"I know an M.D., i.e. a doctor. Like Dr. Smith, a.k.a. Docsmith.\"\n",
    "hard_split_text = [\"I know an M.D., i.e. a doctor.\",\n",
    "                   \"Like Dr. Smith, a.k.a. Docsmith.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take home exercise:\n",
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "def sentencer3(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if map(str.strip, sentencer3(hard_text)) == hard_split_text:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print sentencer3(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "Sentence segmentation is harder than it seems! Let's take a look at how a modern system does it. [NLTK](http://www.nltk.org) is the most widely-used NLP library in Python. It [relies on a statistical language model](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt) to determine when to split sentences. You'll notice that even this model can't handle our hard sentences.\n",
    "\n",
    "To get started, you have to download the right dataset. **DO NOT** download everything. It will take forever. When the download window pops up (probably behind your other windows, annoyingly) click on the 'Models' tab, choose the 'punkt' dataset, and just download that."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n",
      "did you read the instructions?\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# download the Punkt Tokenizer Models.\n",
    "# DON'T DOWNLOAD EVERYTHING!\n",
    "# The download window will probably pop up behind your other windows.\n",
    "# uncomment the download command and comment out the print statement when you've understood these instructions.\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "print \"did you read the instructions?\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went to the zoo today.', 'What do you think of that?', 'I bet you hate it!', \"Or maybe you don't\"]\n",
      "['My name is Dr. Lee.', 'There is also a Mrs. Lee.', 'Actually, there are tons!', \"They're other people's wives.\"]\n",
      "['I know an M.D., i.e.', 'a doctor.', 'Like Dr. Smith, a.k.a.', 'Docsmith.']\n"
     ]
    }
   ],
=======
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "print sent_detector.sentences_from_text(easy_text)\n",
    "print sent_detector.sentences_from_text(med_text)\n",
    "print sent_detector.sentences_from_text(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n",
    "\n",
    "A more common task is to ignore sentences and just split text into words. We call this tokenization. Try your hand at this. This task is much easier now that you're familiar with all the string methods. right?? You should be able to write a fairly simple function that can tokenize all of our texts from before."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our objective tokenizations. Note that we've removed some punctuation.\n",
    "easy_words = ['I', 'went', 'to', 'the', 'zoo', 'today',\n",
    "              'What', 'do', 'you', 'think', 'of', 'that',\n",
    "              'I', 'bet', 'you', 'hate', 'it',\n",
    "              'Or', 'maybe', 'you', \"don't\"]\n",
    "med_words = ['My', 'name', 'is', 'Dr', 'Lee',\n",
    "             'There', 'is', 'also', 'a', 'Mrs', 'Lee',\n",
    "             'Actually,', 'there', 'are', 'tons',\n",
    "             \"They're\", 'other', \"people's\", 'wives']\n",
    "hard_words = ['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor',\n",
    "              'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# define a function to split a string into words.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def tokenizer(text):\n",
    "   '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
    "   cleaned_text = text.replace('.', '').replace('?', '').replace('!', '')\n",
    "   return cleaned_text.split()\n"
=======
    "# define a function to split a string into sentences.\n",
    "# If you're familiar with regexes, feel free to use the re module\n",
    "\n",
    "def tokenizer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    return None"
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(easy_text) == easy_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'know',\n",
       " 'an',\n",
       " 'MD,',\n",
       " 'ie',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'Like',\n",
       " 'Dr',\n",
       " 'Smith,',\n",
       " 'aka',\n",
       " 'Docsmith']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(hard_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(med_text) == med_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(hard_text) == hard_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Your version:'\n",
    "    print tokenizer(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization continued\n",
    "\n",
    "Let's see how NLTK [tokenizes text into words](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'know',\n",
       " 'an',\n",
       " 'MD,',\n",
       " 'ie',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'Like',\n",
       " 'Dr',\n",
       " 'Smith,',\n",
       " 'aka',\n",
       " 'Docsmith']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(hard_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'do', \"n't\"]\n",
      "['My', 'name', 'is', 'Dr.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'re\", 'other', 'people', \"'s\", 'wives', '.']\n",
      "['I', 'know', 'an', 'M.D.', ',', 'i.e', '.', 'a', 'doctor', '.', 'Like', 'Dr.', 'Smith', ',', 'a.k.a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "print word_tokenize(easy_text)\n",
    "print word_tokenize(med_text)\n",
    "print word_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It behaves a little differently, and sometimes erratically. Let's try a version based on pattern-matching."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'don', \"'\", 't']\n",
      "['My', 'name', 'is', 'Dr', '.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs', '.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'\", 're', 'other', 'people', \"'\", 's', 'wives', '.']\n",
      "['I', 'know', 'an', 'M', '.', 'D', '.,', 'i', '.', 'e', '.', 'a', 'doctor', '.', 'Like', 'Dr', '.', 'Smith', ',', 'a', '.', 'k', '.', 'a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "print wordpunct_tokenize(easy_text)\n",
    "print wordpunct_tokenize(med_text)\n",
    "print wordpunct_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, there isn't really a right or wrong way to tokenize words. Sometimes punctuation provides valuable semantic content. Sometimes, you want to strip it all away.\n",
    "\n",
    "As a final thought, what do you suppose the following functions do? Go ahead and play with them."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": null,
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'went')\n",
      "('went', 'to')\n",
      "('to', 'the')\n",
      "('the', 'zoo')\n",
      "('zoo', 'today')\n",
      "('today', '.')\n",
      "('.', 'What')\n",
      "('What', 'do')\n",
      "('do', 'you')\n",
      "('you', 'think')\n",
      "('think', 'of')\n",
      "('of', 'that')\n",
      "('that', '?')\n",
      "('?', 'I')\n",
      "('I', 'bet')\n",
      "('bet', 'you')\n",
      "('you', 'hate')\n",
      "('hate', 'it')\n",
      "('it', '!')\n",
      "('!', 'Or')\n",
      "('Or', 'maybe')\n",
      "('maybe', 'you')\n",
      "('you', 'do')\n",
      "('do', \"n't\")\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "source": [
    "for bigram in bigrams(word_tokenize(easy_text)):\n",
    "    print bigram"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'went', 'to')\n",
      "('went', 'to', 'the')\n",
      "('to', 'the', 'zoo')\n",
      "('the', 'zoo', 'today')\n",
      "('zoo', 'today', '.')\n",
      "('today', '.', 'What')\n",
      "('.', 'What', 'do')\n",
      "('What', 'do', 'you')\n",
      "('do', 'you', 'think')\n",
      "('you', 'think', 'of')\n",
      "('think', 'of', 'that')\n",
      "('of', 'that', '?')\n",
      "('that', '?', 'I')\n",
      "('?', 'I', 'bet')\n",
      "('I', 'bet', 'you')\n",
      "('bet', 'you', 'hate')\n",
      "('you', 'hate', 'it')\n",
      "('hate', 'it', '!')\n",
      "('it', '!', 'Or')\n",
      "('!', 'Or', 'maybe')\n",
      "('Or', 'maybe', 'you')\n",
      "('maybe', 'you', 'do')\n",
      "('you', 'do', \"n't\")\n"
     ]
    }
   ],
   "source": [
    "for trigram in trigrams(word_tokenize(easy_text)):\n",
    "    print trigram"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> f760593f951413b73eefceb8166f48c6b7558a0a
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
